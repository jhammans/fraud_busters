{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.x  from https://www.mongodb.com/try/download/database-tools and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.5.1'\n",
        "spark_version = 'spark-3.5.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tHWluTjdoEmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType,StructField,StringType, DateType, IntegerType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ],
      "metadata": {
        "id": "Ik92s2qWoiFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kartik2112/fraud-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "# List all files in the downloaded directory\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)\n"
      ],
      "metadata": {
        "id": "DMKF-SQi30Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "kYWegO8OnBiP"
      },
      "outputs": [],
      "source": [
        "# File location and type\n",
        "file_location = path + \"/fraudTrain.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JcjKV55mnBiQ"
      },
      "outputs": [],
      "source": [
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"fraudTrain\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "implicitDf": true,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b5f66379-6f7f-42ec-8e82-d0e0926a1721",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "sONFucLanBiQ"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"select *\n",
        "            from fraudTrain\"\"\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0b434c2e-a0c7-4b13-b051-12595fcdef30",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nJb7HD_onBiR"
      },
      "outputs": [],
      "source": [
        "# Write DataFrame to Parquet with partitioning by a column (e.g., 'Class')\n",
        "df.write.mode(\"overwrite\").partitionBy(\"is_fraud\").parquet(\"fraud_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ghXVdJ3znBiS"
      },
      "outputs": [],
      "source": [
        "# Read in our new parquet formatted data\n",
        "p_df=spark.read.parquet('fraud_train')\n",
        "p_df.createOrReplaceTempView('p_fraudTrain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aae5d0ee-c7b3-490d-bbbc-9b331119a966",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HTe3IjE_nBiS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "spark.sql(\"\"\"select is_fraud,\n",
        "                    round(avg(amt),2),\n",
        "                    round(min(amt),2),\n",
        "                    round(max(amt),2),\n",
        "                    round(count(amt),2)\n",
        "            from p_fraudTrain\n",
        "            group by is_fraud\"\"\").show(truncate=False)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a2a51479-dbef-4004-9c31-5ad74c91cb13",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "odyn0GVNnBiS"
      },
      "outputs": [],
      "source": [
        " # Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"select cc_num, amt, zip, lat, long, city_pop, unix_time, merch_lat, merch_long, is_fraud from p_fraudTrain\"\n",
        "spark_df = spark.sql(query)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "pandas_df = spark_df.toPandas()\n",
        "\n",
        "# Display the Pandas DataFrame\n",
        "pandas_df.head()"
      ],
      "metadata": {
        "id": "8AJIlPuTc-cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5ee5241f-22cd-4096-a7a8-21f28d08188c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "9JDMBfYAnBiT"
      },
      "outputs": [],
      "source": [
        "# Extract the 'is_fraud' column as a list\n",
        "y = p_df.select(\"is_fraud\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Drop the \"is_fraud\" column and convert the features to a Pandas DataFrame or NumPy array\n",
        "X = pandas_df.drop(\"is_fraud\", axis=1)\n",
        "\n",
        "# Use train_test_split from Scikit-learn\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ac82684e-8ca4-4c40-8213-8011785c45be",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "qQ54UC9InBiT"
      },
      "outputs": [],
      "source": [
        "# Preprocess numerical data for neural network\n",
        "\n",
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "433482b8-8c51-46bc-90bc-0560069e39b0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "vF1EVz1jnBiU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(type(X_train_scaled))  # Should be <class 'numpy.ndarray'>\n",
        "print(type(y_train))         # Should be <class 'numpy.ndarray'>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "442a82be-20bc-4ed9-967a-b37a0547a116",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "j056G-xrnBiV"
      },
      "outputs": [],
      "source": [
        " # Define the deep learning model\n",
        "n_features = X_train_scaled.shape[1]\n",
        "\n",
        "nn_model = tf.keras.models.Sequential()\n",
        "nn_model.add(tf.keras.layers.Input(shape=(n_features,)))\n",
        "nn_model.add(tf.keras.layers.Dense(units=18, activation=\"relu\"))\n",
        "nn_model.add(tf.keras.layers.Dense(units=9, activation=\"relu\"))\n",
        "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9ac59086-4538-43ce-81d6-12c6496c12bb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rhI0kqDQnBiV"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 1227615999916769,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "credit_card_parquet",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}