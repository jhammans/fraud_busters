{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Find the latest version of spark and enter as the spark version\n",
        "# spark_version = 'spark-3.5.1'\n",
        "spark_version = 'spark-3.5.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "tHWluTjdoEmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pyspark packages\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType,StructField,StringType, DateType, IntegerType\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ],
      "metadata": {
        "id": "Ik92s2qWoiFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version of Kaggle dataset\n",
        "path = kagglehub.dataset_download(\"kartik2112/fraud-detection\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "\n",
        "# List all files in the downloaded directory\n",
        "files = os.listdir(path)\n",
        "print(\"Files in the dataset:\", files)\n"
      ],
      "metadata": {
        "id": "DMKF-SQi30Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "kYWegO8OnBiP"
      },
      "outputs": [],
      "source": [
        "# File location and type\n",
        "file_location = path + \"/fraudTrain.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "JcjKV55mnBiQ"
      },
      "outputs": [],
      "source": [
        "# Create a view or table\n",
        "temp_table_name = \"fraudTrain\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)\n",
        "\n",
        "spark.sql(\"\"\"select * from fraudTrain\"\"\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert unix_time to timestamp to see if it is the same as trans_date_trans_time\n",
        "spark.sql(\"\"\"\n",
        "SELECT\n",
        "    cc_num,\n",
        "    unix_time,\n",
        "    trans_date_trans_time,\n",
        "    from_unixtime(unix_time + 220924800) unix_convert,\n",
        "    is_fraud,\n",
        "    CASE\n",
        "        WHEN from_unixtime(unix_time + 220924800) = trans_date_trans_time THEN 'Match'\n",
        "        ELSE 'Mismatch'\n",
        "    END as comparison_result\n",
        "FROM fraudTrain\n",
        "order by cc_num,trans_date_trans_time, is_fraud\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "id": "bYW4mfO2OmL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0b434c2e-a0c7-4b13-b051-12595fcdef30",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nJb7HD_onBiR"
      },
      "outputs": [],
      "source": [
        "# Write DataFrame to Parquet with partitioning by a column (e.g., 'is_fraud')\n",
        "df.write.mode(\"overwrite\").partitionBy(\"is_fraud\").parquet(\"fraud_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ghXVdJ3znBiS"
      },
      "outputs": [],
      "source": [
        "# Read in our new parquet formatted data\n",
        "p_df=spark.read.parquet('fraud_train')\n",
        "p_df.createOrReplaceTempView('p_fraudTrain')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aae5d0ee-c7b3-490d-bbbc-9b331119a966",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "HTe3IjE_nBiS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Get min, max, avg amounts and counts of transactions\n",
        "start_time = time.time()\n",
        "spark.sql(\"\"\"select is_fraud,\n",
        "                    round(avg(amt),2),\n",
        "                    round(min(amt),2),\n",
        "                    round(max(amt),2),\n",
        "                    round(count(amt),2)\n",
        "            from p_fraudTrain\n",
        "            group by is_fraud\"\"\").show(truncate=False)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from geopy.distance import geodesic  # To calculate distance\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "AyagWm9fSDuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_query = \"select cc_num, amt, trans_date_trans_time, lat, long, merchant, merch_lat, merch_long from p_fraudTrain where is_fraud == 1\"\n",
        "spark_fraud_df = spark.sql(fraud_query)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "fraudulent_transactions_df = spark_fraud_df.toPandas()\n",
        "\n",
        "# Display the Pandas DataFrame\n",
        "fraudulent_transactions_df.head()"
      ],
      "metadata": {
        "id": "RttstDoUSPdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_distance(row):\n",
        "    transaction_coords = (row[\"lat\"], row[\"long\"])\n",
        "    merchant_coords = (row[\"merch_lat\"], row[\"merch_long\"])\n",
        "    return geodesic(transaction_coords, merchant_coords).km\n",
        "\n",
        "# Add a distance column\n",
        "fraudulent_transactions_df[\"distance_km\"] = fraudulent_transactions_df.apply(calculate_distance, axis=1)\n",
        "\n",
        "fraudulent_transactions_df.head()\n"
      ],
      "metadata": {
        "id": "HXUAN3fNS0wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique credit card numbers for the dropdown\n",
        "cc_nums = fraudulent_transactions_df[\"cc_num\"].unique()\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=cc_nums,\n",
        "    description=\"Credit Card:\",\n",
        "    value=cc_nums[0],\n",
        ")\n",
        "\n",
        "# Function to create the map for a selected credit card number\n",
        "def create_map(cc_num):\n",
        "    filtered_df = fraudulent_transactions_df[fraudulent_transactions_df[\"cc_num\"] == cc_num]\n",
        "\n",
        "    # Initialize the map\n",
        "    if not filtered_df.empty:\n",
        "        map_center = [filtered_df.iloc[0][\"lat\"], filtered_df.iloc[0][\"long\"]]\n",
        "        fraud_map = folium.Map(location=map_center, zoom_start=10)\n",
        "\n",
        "        # Add markers and lines\n",
        "        for _, row in filtered_df.iterrows():\n",
        "            # Customer home location\n",
        "            folium.Marker(\n",
        "                location=[row[\"lat\"], row[\"long\"]],\n",
        "                popup=f\"Customer\",\n",
        "                icon=folium.Icon(color=\"blue\"),\n",
        "            ).add_to(fraud_map)\n",
        "\n",
        "            # Merchant location\n",
        "            folium.Marker(\n",
        "                location=[row[\"merch_lat\"], row[\"merch_long\"]],\n",
        "                popup=f\"Merchant: {row['merchant']}<br>Distance: {0.62137 * row['distance_km']:.2f} miles<br>Amount: ${row['amt']:,.2f}\",\n",
        "                icon=folium.Icon(color=\"green\"),\n",
        "            ).add_to(fraud_map)\n",
        "\n",
        "            # Draw a line between customer home and merchant\n",
        "            folium.PolyLine(\n",
        "                locations=[(row[\"lat\"], row[\"long\"]), (row[\"merch_lat\"], row[\"merch_long\"])],\n",
        "                color=\"red\",\n",
        "                weight=2,\n",
        "            ).add_to(fraud_map)\n",
        "\n",
        "        # Display the map\n",
        "        return fraud_map\n",
        "    else:\n",
        "        return folium.Map(location=[0, 0], zoom_start=2)\n",
        "\n",
        "# Function to update the map when the dropdown changes\n",
        "def update_map(change):\n",
        "    selected_cc_num = change[\"new\"]\n",
        "    map_display.clear_output()\n",
        "    with map_display:\n",
        "        fraud_map = create_map(selected_cc_num)\n",
        "        # fraud_map.save(\"fraud_map.html\")\n",
        "        display(fraud_map)\n",
        "\n",
        "# Display the map with the initial credit card number\n",
        "map_display = widgets.Output()\n",
        "with map_display:\n",
        "    display(create_map(cc_nums[0]))\n",
        "\n",
        "# Update the map when a new credit card number is selected\n",
        "dropdown.observe(update_map, names=\"value\")\n",
        "\n",
        "# Display the dropdown and map\n",
        "display(dropdown, map_display)\n"
      ],
      "metadata": {
        "id": "4ajYbQiWS9RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract hour and day\n",
        "fraudulent_transactions_df[\"hour\"] = fraudulent_transactions_df[\"trans_date_trans_time\"].dt.hour\n",
        "fraudulent_transactions_df[\"weekday\"] = fraudulent_transactions_df[\"trans_date_trans_time\"].dt.day_name()\n",
        "\n",
        "# Define the order of weekdays\n",
        "weekday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "\n",
        "# Aggregate the count of frauds by weekday and hour\n",
        "heatmap_data = fraudulent_transactions_df.groupby([\"weekday\", \"hour\"]).size().reset_index(name=\"count\")\n",
        "\n",
        "# Pivot for heatmap format\n",
        "heatmap_pivot = heatmap_data.pivot(index=\"weekday\", columns=\"hour\", values=\"count\").fillna(0).reindex(weekday_order)\n",
        "\n",
        "# Plot the heat map\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(heatmap_pivot, annot=True, fmt=\".0f\", cmap=\"YlGnBu\", cbar_kws={'label': 'Fraud Count'})\n",
        "plt.title(\"Heat Map of Fraudulent Transactions by Weekday and Hour\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Weekday\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8mCRF63P61P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a2a51479-dbef-4004-9c31-5ad74c91cb13",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "odyn0GVNnBiS"
      },
      "outputs": [],
      "source": [
        " # Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"select cc_num, amt, zip, lat, long, city_pop, unix_time, merch_lat, merch_long, is_fraud from p_fraudTrain\"\n",
        "spark_df = spark.sql(query)\n",
        "\n",
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "pandas_df = spark_df.toPandas()\n",
        "\n",
        "# Display the Pandas DataFrame\n",
        "pandas_df.head()"
      ],
      "metadata": {
        "id": "8AJIlPuTc-cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5ee5241f-22cd-4096-a7a8-21f28d08188c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "9JDMBfYAnBiT"
      },
      "outputs": [],
      "source": [
        "# Extract the 'is_fraud' column as a list\n",
        "y = p_df.select(\"is_fraud\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Drop the \"is_fraud\" column and convert the features to a Pandas DataFrame or NumPy array\n",
        "X = pandas_df.drop(\"is_fraud\", axis=1)\n",
        "\n",
        "# Use train_test_split from Scikit-learn\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ac82684e-8ca4-4c40-8213-8011785c45be",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "qQ54UC9InBiT"
      },
      "outputs": [],
      "source": [
        "# Preprocess numerical data for neural network\n",
        "\n",
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "433482b8-8c51-46bc-90bc-0560069e39b0",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "vF1EVz1jnBiU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(type(X_train_scaled))  # Should be <class 'numpy.ndarray'>\n",
        "print(type(y_train))         # Should be <class 'numpy.ndarray'>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "442a82be-20bc-4ed9-967a-b37a0547a116",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "j056G-xrnBiV"
      },
      "outputs": [],
      "source": [
        " # Define the deep learning model\n",
        "n_features = X_train_scaled.shape[1]\n",
        "\n",
        "nn_model = tf.keras.models.Sequential()\n",
        "nn_model.add(tf.keras.layers.Input(shape=(n_features,)))\n",
        "nn_model.add(tf.keras.layers.Dense(units=18, activation=\"relu\"))\n",
        "nn_model.add(tf.keras.layers.Dense(units=9, activation=\"relu\"))\n",
        "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9ac59086-4538-43ce-81d6-12c6496c12bb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "rhI0kqDQnBiV"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "environmentMetadata": null,
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 1227615999916769,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "credit_card_parquet",
      "widgets": {}
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}